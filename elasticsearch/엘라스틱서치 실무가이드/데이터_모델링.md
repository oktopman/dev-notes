## 매핑 API 이해하기
### 매핑인덱스 만들기
- 매핑은 색인시 데이터가 어디에 어떻게 저장될지를 결정하는 설정. 인덱스에 추가되는 각 데이터 타입을 구체적으로 정의하는 일
- 엘라스틱서치는 기본적으로 스키마리스이기 때문에 매핑 설정없이 색인하면 원하지이않는 타입이 설정될수있음. 실수로 잘못된 타입이 지정될 경우 수정할 방법이 없다.
- 분석가능한 타입은 text, 해당 정보를 그대로 보여주기만 할 필드는 특성에따라 keyword, integer 타입 설정
- 필드는 계층구조를 가질수 있음
```
"mapping": {
    "_doc": {
        "directors": {
            "properties": {
                "peopleNm": {
                    "type": "keyword"
                }
            }
        }
    }
}
```
### 매핑 확인
- ES에서 제공하는 mapping API 로 생성한 매핑정보를 확인가능
```
GET movie_search/_mapping
```
### 매핑 파라미터
- 매핑 파라미터는 색인할 필드의 데이터를 어떻게 저장할지에 대한 다양한 옵션 제공
- analyzer, normalizer, boost, properties...  등등

### 메타 필드
- 메타 필드는 ES 에서 생성한 문서에서 제공하는 특별한 필드
- _index : 해당 문서(document)가 속한 인덱스의 이름을 담고있음
- _type, _uid, _all
- _id : 문서를 식별하는 유일한 키값
- _source : 문서의 원본 데이터를 제공. _reindex API 를 이용해 다른 인덱스에 재색인 가능
- _routing : 특정 문서를 특정 샤드에 저장하기 위해 사용자가 지정하는 메타 필드. 별도설정없이 색인시 문서는 수식에 따라 샤드에 골고루 분산 저장
```
Hash (document_id % num_of_shards)
```
특정 문서를 하나의 샤드에 저장하고 싶을때 _routing 메타 필드 사용. _routing 메타 필드 사용하면 색인시 해당 문서들을 동일한 라우팅 ID로 지정
```
// 색인
PUT test_index/_doc/1?routing=ko
{
    "name": "oktop"
}

// 조회
GET test_index/_search
{
  "query": {
    "terms": {
      "_routing": ["ko"] // 여러개 routing 값 조회 가ㅇ 
    }
  }
}
```

## 엘라스틱서치 분석기
### 텍스트 분석 개요
- ES 는 루씬 기반으로 구축된 텍스트 기반 검색 엔진
- ES 는 문서를 색인하기 전에 해당 문서의 필드 타입이 무엇인지 확인하고 텍스트타입이면 분석기를 이용해 분석

### 역색인 구조
- 모든 문서가 가지는 단어의 고유 단어 목록
- 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
- 전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보
- 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도
- 색인 한다는것은 역색인 파일을 만든다는것. 색인할 때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정을 분석(analyze)라고 함

### 분석기의 구조
- 분석기는 다음과 같은 프로세스로 동작
1. 문장을 특정한 규칙에 의해 수정
2. 수정한 문장을 개별 토큰으로 분리
3. 개별 토큰을 특정한 규칙에 의해 변경

- Character Filter
  - 문장을 분석하기전 입력 텍스트에 의해 특정단어를 변경하거나 HTML 과 같은 태그를 제거하는 역할을 하는 필터
- Tokenizer Filter
  - 텍스트를 어떻게 나눌지 정의. 상황에 맞는 적절한 토크나이저 사용가능
- Token Filter
  - 토큰화된 단어를 필터링하여 원하는 토큰으로 변환. 예를들어 불필요한 단어를 제거하거나 동의어사전, lowercase 등 작업 수행가능함

1. 문장이 Character Filter 를 통해 가공
2. 가공된 문장이 Tokenizer Filter 를 통해 텀으로 분리됨
3. 분리된 토큰들이 Token Filter 를 통해 사용자가 원하는 형태로 변환됨
4. 색인
```
PUT /movie_index
{
    "settings": {
        "index": {
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    },
    "analysis": {
        "analyzer": {
            "custom_movie_analyzer": {
                "type: "custom",
                "char_filter": [
                    "html_strip"
                ],
                "tokenizer": "standard",
                "filter": [
                    "lowercase"
                ]
            }
        }
    }
}
```
![image](https://user-images.githubusercontent.com/55048593/155543037-10a23276-0589-4ab5-b043-50b3040ab5b2.png)

### 분석기 사용법
- 분석기를 이용한 분석
  - ES 에서 형태소가 어떻게 분석되는지는 _analyze API 를 통해 확인 가능
```
GET my_index/_analyze
{
  "analyzer": "custom_movie_analyzer",
  "text" : "캐리비안의 해적"
}
```
- 필드를 이용한 분석
  - 인덱스 설정시 필드에 분석기를 직접 설정가능
- 색인과 검색시 분석기를 각각 설정 가능
```
...
"analyzer":{
  "movie_lower_test_analyzer":{
    "type":"custom",
    "tokenizer":"standard",
    "filter":[ "lowercase" ]
  },
  "movie_stop_test_analyzer":{
    "type":"custom",
    "tokenizer":"standard",
    "filter":[ "lowercase", "english_stop" ]
  }
},
...
"mapping":{
  ...
  "properties":{
    "title":{
      "type":"text",
      "analyzer":"movie_stop_test_analyzer",
      "search_analyer":"movie_lower_test_analyzer"
    }
  }
}
```
- 색인시는 `movie_stop_test_analyzer` 분석기가 동작하게하고 검색시는 `movie_lower_test_analyzer` 사용
- 분석기 매핑시 기본적으로 `analyzer` 항목으로 설정하면 색인,검색 시점에 동일한 분석기를 사용한다는 의미, 각 시점에 다른 분석기를 사용하려면 `search_analyzer` 항목을 이용해 분석기 재정의 필요
- 두 분석기 모두 standard tokenizer 를사용하고 분리된 토큰을 소문자로 변경
- `movie_stop_test_analyzer` 는 추가적으로 불용어(stopword)를 처리
```
// 색인
PUT my_index/_doc/1
{
  "title": "Harry Potter and the Chamber of Secrets"
}
```
색인시점에 `movie_stop_test_analyzer` 를 사용하도록 설정했기때문에 다음과 같이 불용어 제거 및 lowercase 되어 토큰화됨
```
[harry], [potter], [chamber], [secrets]
```
검색시에는 불용어처리 설정을 하지않았기때문에 불용어를 포함하여 토큰을 나눌것이다.
### 대표적인 분석기(Analyzer)
ES 에서는 기본 분석기를 제공한다.  
- Standard
  - 인덱스 생성시 settings에 analyzer를 정의하는데, 아무런 정의 하지않고 필드타입이 text 일때 기본적으로 standard analyzer 를 사용
  - 공백 혹은 특수기호를 기준으로 토큰 분리, 모든 문자를 소문자로 변경하는 토큰 필터를 사용
- Whitespace
  - 공백 문자열을 기준으로 토큰을 분리하는 분석기
- Keyword
  - 전체 문자열을 하나의 키워드처럼 처리. 토큰화 작업 X

### 전처리 필터
- 전처리 필터는 데이터 정제역할을 하지만 토크나이저 내부에서도 전처리가 가능하기때문에 상대적으로 활용도가 떨어짐
### 토크나이저 필터
- 분석기를 구성하는 핵심요소. 전처리 필터를 거쳐 토크나이저 필터로 문서가 넘어오면 해당 텍스트는 tokenizer 의 특성에 맞게 적절히 분해
- Standard 토크나이저
  - 일반적으로 사용하는 토크나이저이며 대부분의 기호를 만나면 토큰으로 나눔
```
//색인
GET movie_index/_analyze
{
  "tokenizer":"standard",
  "text":"Harry Potter and the Chamber of Secrets"
}

// 결과
[Harry, Potter, and, the, Chamber, of, Secrets]
```
- Whitespace tokenizer
  - 공백을 만나면 텍스트를 토큰화 한다
- Ngram 토크나이저
- Edge Ngram 토크나이저
- Keyword 토크나이저
### 토큰 필터
- 토크나이저에서 분리된 토큰들을 변형하거나 추가,삭제 할때 사용되는 필터. 토큰이 분리되야 사용가능하기때문에 독립적으로 사용불가
- Ascii Folding 토큰 필터
  - 아스키코드에 해당하지않는경우 문자를 ASC 요소로 변경
```
PUT movie_af_analyzer
{
  "settings":{
    "analysis":{
      "analyzer":{
        "asciifolding_analyzer":{
          "tokenizer":"stnadrad",
            "asciifolding_analyzer": {
              "filter": [ "stnadard", "asciifolding" ]
            }
        }
      }
    }
  }
}
```
- lowercase 토큰 필터
  - 토큰을 구성하는 전체 문자열을 소문자로 변환
- uppercase 토큰 필터
- stop 토큰 필터
  - 불용어로 등록할 사전을 구축해서 사용하는 필터
  - stopwords : 불용어를 매핑에 직접 등록해 사용
  - stopwords_path : 불용어 사전이 존재하는 경로에 지정. es 서버가 있는 config 폴더에 생성
  - ignore_case : true 로 지정할 경우 모든 단어를 소문자로 변경하여 저장
```
PUT movie_stop_analyzer
{
  "settings":{
    "analysis":{
      "analyzer":{
        "stop_filter_analyzer":{
          "tokenizer":"stnadrad",
              "filter": [ "stnadard", "stop_filter" ]
            }
        }
      },
      "filter": {
        "stop_filter": {
          "type": "stop",
          "stopwords": [ "and", "is", "the" ]
        }
      }
    }
  }
}
```
- stemmer 토큰 필터
  - stemming 알고리즘을 사용해 토큰 변형하는 필터
- synonym 토큰 필터(뒤에서 자세히 다룸)
  - 동의어를 처리할수 있는 필터
- Trim 토큰 필터
## 동의어 사전
- 토큰 필터중에 synonym 을 사용하면 동의어 처리가 가능해짐
- 동의어는 검색 기능을 풍부하게 할수있게 도와주는 도구. 원문에 특정 단어가 존재하지않더라도 색인 데이터를 토큰화해서 저장할때 동의어나 유의어에 해당하는 단어를 함께 저장해서 검색이 가능해지게 하는 기술
- 예를들어 `Elasticsearch` 라는단어가 인덱스에 저장됭어있을때 `엘라스틱서치`라고 검색하면 검색되지않는다. 하지만 동의어 등록을 통해 검색 가능
- 매핑 설정 정보에 동의어를 등록하는방식과 특정 파일로 별도로 생성해서 관리하는 방식이 있는데 실무에서는 주로 파일로 관리
- 검색에서 다루는 분야가 많아질수록 동의어의 수도 늘어나고 분야별로 파일도 늘어나는데, 그 안에 동의어 변환 규칙도 많아질것. 실무에서는 이러한 동의어를 모아둔 파일들을 `동의어 사전` 이라고 함

### 동의어 사전만들기
- ES가 설치된 서버 아래의 config 디렉터리에 파일로 관리해야함
```
ES 설치 디렉터리/config/analysis/xxx-synonym.txt
```
### 동의어 추가
- 쉼표로 분리하여 동의어를 추가 가능
`Elasticsearch,엘라스틱서치`
- 동의어 처리기준은 토큰 필터의 종류가 무엇이고 어떤 작업을 했느냐에 따라 달라질 수 있음
  - 예를들어 `Elasticsearch` 로 토큰분리후 lowercase 필터를 실행했다면 `elasticsearch` 로 토큰이 변경될것이기때문에 동의어로 등록한 `Elasticsearch` 와 일치하지않아서 다른 토큰으로 인식해서 동의어가 적용안될것
    - 최신버전에는 대소문자가 사전에 등록된 단어와 일치하지않더라도 자동으로 인식해서 동의어 처리한다고함

### 동의어 치환
- 치환을 하면 원본 토큰이 제거되고 변경될 새로운 토큰이 추가됨
```
Elasticsearh, 엘라스틱서치
Harry => 해리
```
- 동의어사전은 실시간이아니므로 수정된 동의어 적용이 필요하다면 인덱스 reload 필요
- 검색 시점에는 사전내용이 변경되더라도 실시간 반영되고 색인시점은 리로드 필요. 동의어 사전이 빈번히 수정되는 인덱스라면 색인 시점은 적용하지않고 검색시점만 적용하는 방식으로 이러한 문제 해결
```
// 인덱스생성시 동의어 사전등록 설정
PUT my_index
{
  "settings":{
    "analysis":{
      "analyzer":{
        "stop_filter_analyzer":{
          "tokenizer":"stnadrad",
              "filter": [ "lowercase", "synonym_filter" ]
            }
        }
      },
      "filter": {
        "synonym_filter": {
          "type": "synonym",
          "ignore_case": true,
          "synonyms_path": "analysis/synonym.txt"
        }
      }
    }
  }
}

// 문장 테스트
POST my_index/_analyze
{
  "analyzer":"synonym_analyzer",
  "text": "Elasticsearch Harry Potter"
}

// 결과
{
  "tokens": [
    {
      "token": "elasticsearch",
      ...
    },
    {
      "token": "엘라스틱서치",
      ...
    },
    {
      "token": "해리",
      ...
    },
    {
      "token": "potter",
      ...
    }
  ]
}
```
동의어 사전을 수정하고 반영하려면 인덱스를 reload 해야한다. close 하는 동안은 검색도 불가능
```
POST my_index/_close
POST my_index/_open
```

## Document API 이해하기
- 인덱스관리 목적으로 Document API 제공
### 문서 파라미터
- _id : 유니크한 값. ID 지정하지않으면 자동으로 ES 가 UUID 형태로 부여
- _version : 문서의 변경이 일어날때마다 버전값 증가
- op_type
- timeout 설정
- 인덱스 매핑정보 자동 생성
```
action.auto_create_index // 인덱스의 자동 생성 여부 설정. 기본값 true
index.mapper.dynamic // 동적 매핑 사용 여부 설정. 기본값 true
```
### Index API
- 문서를 특정 인덱스에 추가할때 사용
```
PUT my_index/_doc/1
{
  "moviceCd": 12345
}
```
### Get API
- 특정 문서를 인덱스에서 조회시 사용
```
GET my_index/_doc/1
```
### Delete API
- 문서를 삭제할때 사용
```
DELETE my_index/_doc/1 // 특정문서 삭제
DELETE my_index // 인덱스 삭제
```
### Delete By Query API
- 특정인덱스에서 검색수행한이후 그 결과에 해당하는 문서만 삭제
```
POST my_index/_delete_by_query
{
  "query": {
    "term": {
      "moviceCd": 1231232
    }
  }
}
```
위 쿼리를 통해 몇건이 조회되었고, 몇건이 삭제되었는지 자세히 보여줌
### Update API
- 스크립트를 바탕으로 문서 수정가능. 스크립트를 통해 `ctx._source.필드명` 과 같은형태로 접근가능
```
POST my_index/_doc/1/_update
{
  "script": {
    "source": "ctx._source.counter += params.count",
    "lang":, "painless",
    "params": {
      "count": 1
    }
  }
}
```
### Bulk API
- 한번의 API 호출로 다수의 문서를 색인하거나 삭제가능
- 단점은 여러데이터가 한번에 처리되기때문에 실패시 수정된 결과는 롤백되지않음
### Reindex API
- 인덱스에서 다른 인덱스로 문서 복사 가능
- 기본적으로 1000건 단위로 스크롤 수행. size 항목으로 스크롤 크기를 지정해서 많은양의 문서 복사 가능 
## 참고
http://www.yes24.com/Product/Goods/71893929